{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whisper STT Model Testing\n",
    "\n",
    "Deep dive into Whisper speech-to-text performance and capabilities.\n",
    "\n",
    "## Tests covered:\n",
    "- Model loading and initialization\n",
    "- Transcription accuracy\n",
    "- Processing speed benchmarks\n",
    "- Different audio formats and quality levels\n",
    "- Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.insert(0, '/app')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "print(\"✓ Whisper testing notebook ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize STT service\n",
    "from ai.voice.stt_service import STTService\n",
    "\n",
    "print(f\"Whisper Model Size: {os.getenv('WHISPER_MODEL_SIZE', 'base')}\")\n",
    "print(\"\\nAvailable Whisper models:\")\n",
    "print(\"  tiny   - Fastest, least accurate (~75MB)\")\n",
    "print(\"  base   - Balanced (default, ~150MB)\")\n",
    "print(\"  small  - Better accuracy (~500MB)\")\n",
    "print(\"  medium - High accuracy (~1.5GB)\")\n",
    "print(\"  large  - Best accuracy (~3GB)\")\n",
    "\n",
    "# Initialize service\n",
    "stt_service = STTService()\n",
    "print(\"\\n✓ STT Service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Test Audio\n",
    "\n",
    "Create synthetic audio for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test audio using TTS\n",
    "from ai.voice.tts_service import TTSService\n",
    "\n",
    "tts_service = TTSService()\n",
    "\n",
    "test_phrases = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"I need to schedule a consultation with my attorney.\",\n",
    "    \"What are the filing deadlines for this case?\",\n",
    "    \"Please send me the contract review by Friday.\"\n",
    "]\n",
    "\n",
    "test_audio = {}\n",
    "\n",
    "print(\"Generating test audio...\")\n",
    "for i, phrase in enumerate(test_phrases):\n",
    "    audio_data = await tts_service.synthesize(phrase)\n",
    "    if audio_data:\n",
    "        test_audio[i] = {\n",
    "            'text': phrase,\n",
    "            'audio': audio_data\n",
    "        }\n",
    "        print(f\"  ✓ Generated: {phrase[:50]}...\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(test_audio)} test audio samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transcription Accuracy Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test transcription accuracy\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def calculate_similarity(a, b):\n",
    "    \"\"\"Calculate similarity ratio between two strings.\"\"\"\n",
    "    return SequenceMatcher(None, a.lower(), b.lower()).ratio()\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Testing transcription accuracy...\\n\")\n",
    "\n",
    "for i, data in test_audio.items():\n",
    "    original_text = data['text']\n",
    "    audio_data = data['audio']\n",
    "    \n",
    "    # Transcribe\n",
    "    start_time = time.time()\n",
    "    result = await stt_service.transcribe(audio_data)\n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    if result:\n",
    "        transcribed_text = result.get('text', '')\n",
    "        similarity = calculate_similarity(original_text, transcribed_text)\n",
    "        \n",
    "        results.append({\n",
    "            'original': original_text,\n",
    "            'transcribed': transcribed_text,\n",
    "            'similarity': similarity,\n",
    "            'processing_time': processing_time,\n",
    "            'confidence': result.get('confidence', 0)\n",
    "        })\n",
    "        \n",
    "        print(f\"Test {i+1}:\")\n",
    "        print(f\"  Original:    {original_text}\")\n",
    "        print(f\"  Transcribed: {transcribed_text}\")\n",
    "        print(f\"  Similarity:  {similarity*100:.1f}%\")\n",
    "        print(f\"  Time:        {processing_time:.3f}s\")\n",
    "        print()\n",
    "\n",
    "# Calculate averages\n",
    "if results:\n",
    "    avg_similarity = np.mean([r['similarity'] for r in results])\n",
    "    avg_time = np.mean([r['processing_time'] for r in results])\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Average Similarity: {avg_similarity*100:.1f}%\")\n",
    "    print(f\"Average Processing Time: {avg_time:.3f}s\")\n",
    "    print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Audio Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize one of the test audio samples\n",
    "if test_audio:\n",
    "    sample_audio = test_audio[0]['audio']\n",
    "    audio_array = np.frombuffer(sample_audio, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "    \n",
    "    # Assume 22050 Hz sample rate (adjust if needed)\n",
    "    sr = 22050\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Waveform\n",
    "    librosa.display.waveshow(audio_array, sr=sr, ax=axes[0])\n",
    "    axes[0].set_title('Waveform', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Time (s)')\n",
    "    axes[0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Spectrogram\n",
    "    D = librosa.stft(audio_array)\n",
    "    S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "    img = librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='hz', ax=axes[1])\n",
    "    axes[1].set_title('Spectrogram', fontsize=12, fontweight='bold')\n",
    "    fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
    "    \n",
    "    # Mel Spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio_array, sr=sr)\n",
    "    mel_spec_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
    "    img2 = librosa.display.specshow(mel_spec_db, sr=sr, x_axis='time', y_axis='mel', ax=axes[2])\n",
    "    axes[2].set_title('Mel Spectrogram', fontsize=12, fontweight='bold')\n",
    "    fig.colorbar(img2, ax=axes[2], format='%+2.0f dB')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Play audio\n",
    "    display(Audio(audio_array, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark processing times for different audio lengths\n",
    "if results:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot processing times\n",
    "    indices = range(len(results))\n",
    "    processing_times = [r['processing_time'] for r in results]\n",
    "    \n",
    "    plt.bar(indices, processing_times, alpha=0.7, edgecolor='black')\n",
    "    plt.axhline(y=np.mean(processing_times), color='r', linestyle='--', \n",
    "                label=f'Mean: {np.mean(processing_times):.3f}s')\n",
    "    plt.xlabel('Test Sample')\n",
    "    plt.ylabel('Processing Time (seconds)')\n",
    "    plt.title('Whisper Processing Time by Sample', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Custom Audio Testing\n",
    "\n",
    "Test with your own audio files if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom audio file\n",
    "# Uncomment and modify path as needed\n",
    "\n",
    "# custom_audio_path = '/app/cache/audio/your_file.wav'\n",
    "# \n",
    "# if os.path.exists(custom_audio_path):\n",
    "#     with open(custom_audio_path, 'rb') as f:\n",
    "#         audio_data = f.read()\n",
    "#     \n",
    "#     result = await stt_service.transcribe(audio_data)\n",
    "#     \n",
    "#     if result:\n",
    "#         print(f\"Transcription: {result.get('text', '')}\")\n",
    "#         print(f\"Confidence: {result.get('confidence', 'N/A')}\")\n",
    "# else:\n",
    "#     print(\"Custom audio file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook tested the Whisper STT model's:\n",
    "- Transcription accuracy\n",
    "- Processing performance\n",
    "- Audio quality handling\n",
    "\n",
    "Use these insights to optimize your voice processing pipeline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
